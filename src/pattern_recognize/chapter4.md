#### 4.1

(1) 指出从 x 到超平面 $$g(x) = w^T x + w_0 = 0$$ 的距离

$$
r = \frac{\mid g(x)\mid}{\|w\|}
$$

是在 $$g(x_a) = 0$$ 的约束条件下，使 $$\|x - x_a\|^2$$ 达到极小的解；

(2) 指出在超平面上的投影是

$$
x_p = x - \frac {g(x)}{\|w\|^2 }w
$$

解答：

(1) 由于满足约束条件 $$g(x_a) = 0$$ ，所以 $$x_a$$ 必然在超平面上，我们下面来证明，要使 $$\|x - x_a\|^2$$ 达到极小值，则 $$x_a$$ 必须是 $$x$$ 在超平面上的投影。

使用反证法，假如 $$x_a$$ 不是投影，并设投影点为 $$x_p$$ ，于是线段 $$xx_p$$ 必然垂直于线段 $$x_a x_p$$ ，所以线段 $$x_a x$$ 就是直角三角形 $$xx_p x_a$$ 的斜边，且 $$\|x-x_p\| < \|x - x_a\|$$ 。所以得证  $$x_a$$ 必然是 $$x$$ 在超平面上的投影。

又由于 $$x$$ 到超平面的距离为 $$r$$ ，所以

$$
\begin{aligned}
&x = x_a + \frac{w}{\|w\|} r\\
\Rightarrow & \|w\| (x - x_a) = w r\\
\Rightarrow & w^T \|w\|(x- x_a) = w^T w r\\
\Rightarrow & r = \frac{w^T(x-x_a)}{\|w\|}
\end{aligned}
$$

因为 $$x_a$$ 在超平面上，所以 $$w^T x_a = -w_0$$ ，代入上式得到

$$
r = \frac{w^Tx+w_0}{\|w\|} = \frac{g(x)}{\|w\|}
$$

(2) 根据 (1) 的论述，由于

$$
x = x_p+\frac{w}{\|w\|}r
$$

且 $$r = \frac{g(x)}{\|w\|}$$ ，所以

$$
x_p = x - \frac {g(x)}{\|w\|^2 }w
$$

#### 4.2

设有一维空间二次判别函数

$$
g(x) = 5 + 7x + 9 x^2
$$

(1) 试映射成广义齐次线性判别函数；

(2) 总结把高次函数映射成齐次线性函数的方法。

解答：

如果令 $$y_0 = 1, y_1 = x, y_2 = x^2$$ ，且令 $$a_0 = 5, a_1 = 7, a_2 = 9$$ ，那么该二次判别函数就变成

$$
g(y) = a^T y
$$

(2) 把高次函数映射成齐次线性函数的方法就是把每个高次项当作一个新的变量看待，用空间维度来换变量的幂数。

### 4.3

(1) 通过映射，把一维二次判别函数

$$
g(x) = a_1 + a_2 x + a_3 x^2
$$

映射成三维广义齐次线性判别函数；

(2) 若 $$x$$ 在一维空间具有分布密度 $$p(x)$$ ，说明三维空间中的分布密度退化成只在一条曲线上有值，且曲线上的值无穷大。

解答:

(1) 令

$$
y = \left[
\begin{aligned}
&1\\&x\\&x^2
\end{aligned}
\right]
,\,\,
a = \left[
\begin{aligned}
&a_1\\&a_2\\&a_3
\end{aligned}
\right]
$$

于是可以得到三维广义线性判别函数

$$
g(y) = a^T y
$$

#### 4.4

对于二维线性判别函数


$$
g(x) = x_1 +2 x_2 -2
$$

(1) 将判别函数写成 $$g(x) = w^T x + w_0$$ 的形式，并画出 $$g(x)=0$$ 的几何图形

(2) 映射成广义齐次线性判别函数

$$g(x) = a^T y$$

(3) 指出上述 X 空间实际是 Y 空间的一个子空间，且 $$a^T y = 0$$ 对 X 子空间的划分与原空间中 $$w^T x+ w_0 = 0$$ 对原 X 空间的划分相同，并在图中表示出来

解答：

(1) 令

$$
x = \left[
\begin{aligned}
x_1\\x_2
\end{aligned}
\right]
,\,\,
w= \left[
\begin{aligned}
w_1\\w_2
\end{aligned}
\right]
$$

并且令 $$w_0 = -2$$ ，则可以把函数写成

$$
g(x) = w^T x+ w_0
$$

![](/img/fig4-1.png)

(2) 令

$$
y = \left[
\begin{aligned}
&x_1\\&x_2\\&1
\end{aligned}
\right]
,\,\,
a = \left[
\begin{aligned}
&1\\&2\\&-2
\end{aligned}
\right]
$$

(3)

1. 由于 X 是一个二维向量空间，对于任意的 $$x \in X$$ ，若给它添加第三个分量为 1，那么就有 $$x \in Y$$ ，所以 X 是 Y 的一个子集。

2. 对任意 $$x^{(1)} \in X, x^{(2)} \in X$$ ， 显然有 $$x^{(1)}+x^{(2)} \in X$$ 。

3. 存在 $$[0,0] \in X$$ 使得，$$x + [0,0] = x$$

所以 $$X$$ 必然是 $$Y$$ 的子空间。

#### 4.5

指出在 Fisher 线性判别中， w 的比例因子对 Fisher 判别结果无影响的原因。

解答：

由于 Fisher 准则函数为

$$
J(w) = \frac{w^T S_a w} {w^T S_b w}
$$

如果为 $$w$$ 加上比例因子 $$\eta$$ ，即 $$\etaw$$ ，重新代入上式，可得


$$
J(w) = \frac{\eta w^T S_a \eta w} {\eta w^T S_b  \eta w} =
 \frac{w^T S_a w} {w^T S_b w}
$$

可见，比例因子对判别函数的形式没有影响，也就是说对判别结果无影响。

#### 4.6

证明两向量的外积组成的矩阵一般是奇异的。

解答：

使用数学归纳法，首先，我们证明当向量的维度为 2 时成立，设

$$
a = \left[\begin{aligned}
a_1 \\a_2
\end{aligned}
\right]
,\,\,
b = \left[\begin{aligned}
b_1 \\b_2
\end{aligned}
\right]
$$

那么就有

$$
ab^T = \left[\begin{aligned}
a_1 b_1\quad a_1b_2 \\a_2b_1 \quad a_2b_2
\end{aligned}
\right]
$$

于是可得

$$
\mid ab^T\mid = 0
$$

然后再假设当维度为 n 时，同样成立，则当维度为 n+1 时，两向量外积组成的矩阵行列式等于一些系列代数余子式的线性组合，并且这些代数余子式都是 n x n 阶的，根据假设，它们都等于 0， 所以原行列式等于 0。

#### 4.7

已知 Fisher 准则函数为

$$
J(w) = \frac{w^T S_B w}{w^T S_w w}
$$

(1) 说明是否一般的分数函数都可以用 Lagrange 乘子法求极值解；

(2) 分析 $$J(W)$$ 可用 Lagrange 乘子法求解的条件。

解答：

(1) 一般的分数函数不能，因为 Lagrange 乘子法只能用于求解凸优化问题，如果某个函数具有多有极值点，那么使用 Lagrange 乘子法容易求到局部极值。

#### 4.8

证明在正态等协差条件下，Fisher 线性判别等价于贝叶斯判别。

#### 4.9

(1) 引入余量 b 以后的解区($$a^T y_i \ge b$$) 位于原来的解区 ($$a^T y_i > 0$$) 之中；

(2) 与原解区边界之间的距离为 $$b / \|y_i\|$$

解答：

(1) 对于任意的解满足 $$a^T y_i \ge b$$ 必然有 $$a^T y_i > 0$$

(2) 假设点 $$a^ * $$ 是引入余量后的解向量，并且刚好有 $$a^ {* T} y_i = b$$ ，再假设点 $$a'$$ 是 $$a^ * $$ 在超平面 $$a^T y_i = 0$$ 上的投影，于是原解区边界之间的距离可以由 $$a^ * $$ 到 $$a'$$ 之间的距离表示，设为 $$r$$ 。且有

$$
a^ * = a' + r y_i
$$

两边同时点积 $$y_i$$ 得到

$$
a^ * \cdot y_i = a' \cdot y_i + r y_i\cdot y_i
$$

于是就有 $$b = r\|y_i\|$$ ，所以得证。

#### 4.10

证明，在几何上，感知准则函数正比于被错分类样本到决策面的距离之和。

解答：

设分类超平面的方程为

$$
w^T x + b = 0
$$

那么感知准则函数

$$
f(x) = \sum_{x_i \in \mathcal{E}} (w^T x_i + b) = \|w\|\sum_{x_i \in \mathcal{E}}\left(\frac{w^T}{\|w\|}x_i + \frac{b}{\|w\|}\right)
$$

而显然错分点 $$x_i$$ 到超平面的距离

$$
r_i = \frac{w^T}{\|w\|}x_i + \frac{b}{\|w\|}
$$

所以

$$
f(x) = \|w\|\sum_{x_i \in\mathcal{w}}r_i
$$

即得证。








end


end

end
